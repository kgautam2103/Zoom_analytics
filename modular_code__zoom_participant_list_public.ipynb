{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgautam2103/Zoom_analytics/blob/main/modular_code__zoom_participant_list_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgQ3vGvltzT9"
      },
      "source": [
        "# when you run this code, you will get a link below\n",
        "#Follow the link, copy the code, paste it in the box and then press Enter on the keyboard.\n",
        "#This will authenticate you and will allow you to interact with Google Sheets and other Google apps in your notebook.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_worksheet(work_sheet, columns_list):\n",
        "  work_sheet_data = work_sheet.get_all_values()\n",
        "  work_sheet_data_df = pd.DataFrame(work_sheet_data, columns=columns_list)\n",
        "  if work_sheet_data_df.shape[0] > 0:\n",
        "    work_sheet_data_df = work_sheet_data_df.drop(work_sheet_data_df.index[0])\n",
        "\n",
        "  return work_sheet_data_df\n",
        "\n",
        "def double_encode_uuid(meeting_uuid):\n",
        "  encoded_uuid = urlencode({'id':meeting_uuid})\n",
        "  double_encoded_uuid = urlencode({'id':encoded_uuid[3:]})\n",
        "  return double_encoded_uuid[3:]\n",
        "\n",
        "def get_participants_list(meeting_uid):\n",
        "  get_participants_url = report_meetings+meeting_uid+'/participants?page_size='+page_size\n",
        "  participants_response = requests.get(get_participants_url,headers=auth_headers)\n",
        "  #print(apac_participants_response.json())\n",
        "  participants_list = participants_response.json()['participants']\n",
        "  next_page_token = participants_response.json()['next_page_token']\n",
        "  while next_page_token != \"\":\n",
        "    new_get_participants_url = get_participants_url+'&next_page_token='+next_page_token\n",
        "    new_participants_response = requests.get(new_get_participants_url,headers=auth_headers)\n",
        "    next_page_token =  new_participants_response.json()['next_page_token']\n",
        "    participants_list.extend(new_participants_response.json()['participants'])\n",
        "  return participants_list\n",
        "\n",
        "def get_poll_answer_df(meeting_uid):\n",
        "  poll_response_url = base_url+'report/meetings/'+meeting_uid+'/polls'\n",
        "  poll_response = requests.get(poll_response_url,headers = auth_headers)\n",
        "  response_list = poll_response.json()['questions']\n",
        "  poll_answer_df = json_normalize(response_list,\n",
        "                                record_path=['question_details'],\n",
        "                                meta=['name', 'email',],\n",
        "                                errors='ignore')\n",
        "  return poll_answer_df\n",
        "\n",
        "def get_poll1_df(poll_answer_df, poll_id):\n",
        "  poll1_df=pd.DataFrame()\n",
        "  poll1_df = poll_answer_df[poll_answer_df.polling_id==poll_id]\n",
        "  if poll1_df.shape[0] > 0:\n",
        "    poll1_df  = poll1_df.fillna(\"\")\n",
        "    poll1_df = poll1_df.reset_index()\n",
        "    poll1_df = poll1_df.drop(columns=['question','polling_id','date_time','email','index'])\n",
        "    poll1_df = poll1_df[['name','answer']]\n",
        "  return poll1_df\n",
        "\n",
        "def get_poll2_df(poll_answer_df, poll_id):\n",
        "  poll2_df = pd.DataFrame()\n",
        "  poll2_df = poll_answer_df[poll_answer_df.polling_id==poll_id]\n",
        "  if poll2_df.shape[0] > 0:\n",
        "    poll2_df  = poll2_df.fillna(\"\")\n",
        "    poll2_df['question'] = poll2_df['question'].apply(lambda x: re.sub('^[\\.\\d\\s]*','',x,flags=re.IGNORECASE))\n",
        "    poll2_df.drop_duplicates(subset=['name','question'],inplace=True)\n",
        "    poll2_df = poll2_df.pivot(index=\"name\", columns=\"question\", values=\"answer\")\n",
        "    poll2_df.reset_index(inplace=True)\n",
        "    poll2_df[['answer1','answer2','answer3']] = poll2_df[['Will you like to continue meditation journey with us?']]\n",
        "    poll2_df=poll2_df.drop(columns=['Will you like to continue meditation journey with us?'])\n",
        "  return poll2_df\n",
        "\n",
        "\n",
        "def get_participants_df(participants_list,event,ref_id_list,ref_name_list,ref_email_list,filter_duration):\n",
        "  participants_df=pd.DataFrame()\n",
        "  participants_df = pd.DataFrame(participants_list)\n",
        "  participants_df = participants_df.drop(columns=['attentiveness_score','failover','customer_key'])\n",
        "  participants_df = participants_df[participants_df.duration>filter_duration]\n",
        "  participants_df['id'] = participants_df['id'].apply(lambda x: str(random.randint(1,99999999999)) if x==\"\" else x)\n",
        "  participants_df = participants_df.drop_duplicates(subset=['id'])\n",
        "  participants_df['name'] = participants_df['name'].apply(lambda x: re.sub('^sy[\\.\\s_-]*','',x,flags=re.IGNORECASE))\\\n",
        "  .apply(lambda x: re.sub('[\\.\\s_-]*sy$','',x,flags=re.IGNORECASE))\n",
        "  participants_df = participants_df.drop_duplicates(subset=['name'])\n",
        "  participants_df = participants_df[~participants_df.id.isin(ref_id_list)]\n",
        "  participants_df = participants_df[~(participants_df.name).str.lower().isin(ref_name_list)]\n",
        "  participants_df = participants_df[~(participants_df.user_email).str.lower().isin(ref_email_list)]\n",
        "  participants_df['event'] = event\n",
        "  participants_df['event_date'] = report_datetime\n",
        "  participants_df = participants_df[['event','event_date','id','name','user_email', 'join_time', 'leave_time', 'duration']]\n",
        "  return participants_df\n",
        "\n",
        "def get_rolling_3week_df(rolling_3week_df,final_combined_df):\n",
        "  rolling_3week_df_combined = rolling_3week_df.append(final_combined_df)\n",
        "  rolling_3week_df_combined['event_date'] = pd.to_datetime(rolling_3week_df_combined['event_date'])\n",
        "  rolling_3week_df_final = rolling_3week_df_combined[rolling_3week_df_combined.event_date > report_datetime_3week_before]\n",
        "  rolling_3week_df_final['name']=rolling_3week_df_final['name'].str.lower()\n",
        "  rolling_3week_df_final.drop_duplicates(subset=['event','event_date','name'],inplace=True)\n",
        "  return rolling_3week_df_final\n",
        "\n",
        "def get_weekend_archive_df(weekend_archive_df,final_combined_df):\n",
        "  weekend_archive_df_final = weekend_archive_df.append(final_combined_df)\n",
        "  weekend_archive_df_final['event_date'] = pd.to_datetime(weekend_archive_df_final['event_date'])\n",
        "  weekend_archive_df_final['name']=weekend_archive_df_final['name'].str.lower()\n",
        "  return weekend_archive_df_final\n",
        "\n",
        "def get_weekend_archive_df_final_filtered_3m(weekend_archive_df_final):\n",
        "  weekend_archive_df_final_filtered_3m = weekend_archive_df_final[weekend_archive_df_final.event_date > report_datetime_3month_before]\n",
        "  weekend_archive_df_final_filtered_3m_frequency = weekend_archive_df_final_filtered_3m.groupby(['name']).agg(count=('name',np.count_nonzero))\n",
        "  weekend_archive_df_final_filtered_3m_frequency.reset_index(inplace=True)\n",
        "  weekend_archive_df_final_filtered_3m_frequency.sort_values(by = ['count'],ascending=False,inplace=True)\n",
        "  return weekend_archive_df_final_filtered_3m_frequency\n",
        "\n",
        "def get_archive_group_df_max_date_details(weekend_archive_df_final):\n",
        "  weekend_archive_group_df = weekend_archive_df_final.groupby('name')\n",
        "  archive_max_date = weekend_archive_group_df.agg(event_date=('event_date',np.max))\n",
        "  archive_max_date.reset_index(inplace=True)\n",
        "  archive_max_date_final = archive_max_date[archive_max_date.event_date > report_datetime_3month_before]\n",
        "\n",
        "  archive_group_df_max_date_details = pd.merge(weekend_archive_df_final,archive_max_date_final,how='inner',on=['name','event_date'])\n",
        "  archive_group_df_max_date_details.drop_duplicates(subset=['name','event_date'],inplace=True)\n",
        "  archive_group_df_max_date_details.drop(columns=['join_time','leave_time','duration'],inplace=True)\n",
        "  return archive_group_df_max_date_details\n",
        "\n",
        "\n",
        "def get_updated_sideroom_details(matched_name_list,archive_group_df_max_date_details):\n",
        "  matched_name_df = pd.DataFrame(matched_name_list,columns=['name','matched_names','score'])\n",
        "  matched_name_df = matched_name_df[matched_name_df.score>90]\n",
        "  matched_name_df_final = pd.DataFrame(matched_name_df['matched_names'])\n",
        "  matched_name_df_final.rename(columns={'matched_names':'name'},inplace=True)\n",
        "  matched_name_df_final_details = pd.merge(archive_group_df_max_date_details,matched_name_df_final,how='inner',on='name')\n",
        "\n",
        "  matched_name_df_final_details['event_date'] = matched_name_df_final_details['event_date'].dt.strftime('%Y-%m-%d')\n",
        "  matched_name_df_final_details['last_event']=matched_name_df_final_details['event_date'].str.slice(0,10)+\" \"+matched_name_df_final_details['event']\n",
        "  matched_name_df_final_details.drop(columns=['event','event_date','id'],inplace=True)\n",
        "  matched_name_df_final_details.sort_values(by = ['name'],inplace=True)\n",
        "  return matched_name_df_final_details\n",
        "\n",
        "def pivot_rolling_3week_df(rolling_3week_df_final):\n",
        "  group_name_df = rolling_3week_df_final.groupby(['name'])\n",
        "  count_name = group_name_df.agg(count=('name',np.count_nonzero))\n",
        "  max_date = group_name_df.agg(latest_date=('event_date',np.max) )\n",
        "  max_date.reset_index(inplace=True)\n",
        "  count_name.reset_index(inplace=True)\n",
        "  final_max_count = pd.merge(count_name,max_date,on='name')\n",
        "  final_max_count_filtered = final_max_count[final_max_count['count']>1]\n",
        "  return final_max_count_filtered\n",
        "\n",
        "\n",
        "def get_email_name_ref_df(weekend_archive_df_final):\n",
        "  email_name_ref_df = weekend_archive_df_final[['name','user_email']]\n",
        "  email_name_ref_df.drop_duplicates()\n",
        "  nan_value = float(\"NaN\")\n",
        "  email_name_ref_df.replace(\"\", nan_value, inplace=True)\n",
        "  email_name_ref_df.dropna(inplace=True)\n",
        "  return email_name_ref_df\n",
        "\n",
        "\n",
        "def get_move2side_final(move2side_name_list,final_max_count_filtered):\n",
        "  move2side_name_df = pd.DataFrame(move2side_name_list,columns=['name','matched_names','score'])\n",
        "  move2side_name_df = move2side_name_df[move2side_name_df.score>90]\n",
        "  move2side_name_df_final = pd.DataFrame(move2side_name_df['matched_names'])\n",
        "  move2side_name_df_final_list = move2side_name_df_final['matched_names'].tolist()\n",
        "  final_max_count_filtered = final_max_count_filtered[~final_max_count_filtered.name.isin(move2side_name_df_final_list)]\n",
        "  final_max_count_filtered.drop(columns=['count'],inplace=True)\n",
        "  return final_max_count_filtered"
      ],
      "metadata": {
        "id": "u9ogIvI8Rzb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crh7QbMYyCbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10a422d-e08f-4d9f-fbb9-b170abfde869"
      },
      "source": [
        "import requests\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime,timedelta\n",
        "import gspread\n",
        "import re\n",
        "from pandas import ExcelWriter\n",
        "from google.colab import files\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "from pandas import json_normalize\n",
        "from urllib.parse import urlencode\n",
        "!pip install python-Levenshtein\n",
        "!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import process\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.23.0 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 python-Levenshtein-0.23.0 rapidfuzz-3.6.1\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client_key=\"OIe4uz9xTXK2asGjc8acFw\"\n",
        "\n",
        "client_redirect_url=\"https://meditationjourney.org/\"\n",
        "\n",
        "authorize_url = \"https://zoom.us/oauth/authorize?client_id=\"+client_key+\"&response_type=code&redirect_uri=\"+client_redirect_url\n",
        "\n",
        "print(authorize_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmV2BoujtfRs",
        "outputId": "6a62568b-cffb-4a4b-834a-7ea88ee5bc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://zoom.us/oauth/authorize?client_id=OIe4uz9xTXK2asGjc8acFw&response_type=code&redirect_uri=https://meditationjourney.org/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "client_id='OIe4uz9xTXK2asGjc8acFw'\n",
        "client_secret='nV73LGsazqDc4J4mXoiUWJS2Qp8qsQ9j'\n",
        "secret_string = client_id+':'+client_secret\n",
        "print(secret_string)\n",
        "encode_secret = base64.b64encode(secret_string.encode(\"ascii\"))\n",
        "encode_secret_str = encode_secret.decode(\"ascii\")\n",
        "print(encode_secret_str)\n",
        "\n",
        "authorization = 'Basic ' + encode_secret_str\n",
        "auth_headers = {\n",
        "    'Host': 'zoom.us',\n",
        "    'Authorization': authorization,\n",
        "    'Content-Type': 'application/x-www-form-urlencoded'\n",
        "    }\n",
        "\n",
        "access_code = input(\"please enter the access code recieved \")\n",
        "client_secret = 'nV73LGsazqDc4J4mXoiUWJS2Qp8qsQ9j'\n",
        "grant_type_code = \"authorization_code\"\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "access_url = \"https://zoom.us/oauth/token?grant_type=\"+grant_type_code+\"&redirect_uri=\"+client_redirect_url+\"&code=\"+access_code\n",
        "\n",
        "\n",
        "response = requests.post(access_url,headers=auth_headers)\n",
        "\n",
        "print(response.json())\n",
        "\n",
        "access_token = response.json()[\"access_token\"]\n",
        "refresh_token = response.json()[\"refresh_token\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-2ePVSZA9CE",
        "outputId": "185a08a7-c087-4b72-86db-161c56157f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OIe4uz9xTXK2asGjc8acFw:nV73LGsazqDc4J4mXoiUWJS2Qp8qsQ9j\n",
            "T0llNHV6OXhUWEsyYXNHamM4YWNGdzpuVjczTEdzYXpxRGM0SjRtWG9pVVdKUzJRcDhxc1E5ag==\n",
            "please enter the access code recieved AE6xVMjohRaQeec9iiARper30XlfL9VtQ\n",
            "{'access_token': 'eyJzdiI6IjAwMDAwMSIsImFsZyI6IkhTNTEyIiwidiI6IjIuMCIsImtpZCI6ImY0OTdlMmRjLWFlMTgtNDFjNy1iNWY1LTIyNmNmNThhOTVhYSJ9.eyJ2ZXIiOjksImF1aWQiOiJhMDk1YzNiNjA5Y2JmNDlhMjQzZjAzOTdkM2UxMmE0YSIsImNvZGUiOiJBRTZ4Vk1qb2hSYVFlZWM5aWlBUnBlcjMwWGxmTDlWdFEiLCJpc3MiOiJ6bTpjaWQ6T0llNHV6OXhUWEsyYXNHamM4YWNGdyIsImdubyI6MCwidHlwZSI6MCwidGlkIjowLCJhdWQiOiJodHRwczovL29hdXRoLnpvb20udXMiLCJ1aWQiOiIwNkdEZUJhbFRIcXZhbV9VbS1QMm9RIiwibmJmIjoxNzA2MTIxODQ0LCJleHAiOjE3MDYxMjU0NDQsImlhdCI6MTcwNjEyMTg0NCwiYWlkIjoiM2NwSWxUZ3FTbkdhX0s2S1RDa0VBZyJ9.VUi4naKXk1oJdcKJCueZpNc2LxDbdQphrv9iVpifRp5SCVizARXwZxTRwhxMFFq4qDYxooDSGkhDlVTelYhz5A', 'token_type': 'bearer', 'refresh_token': 'eyJzdiI6IjAwMDAwMSIsImFsZyI6IkhTNTEyIiwidiI6IjIuMCIsImtpZCI6IjIwM2RlNDFiLTliOTEtNGNjMC04MzgyLTJmNmNiMDU4M2ZlYSJ9.eyJ2ZXIiOjksImF1aWQiOiJhMDk1YzNiNjA5Y2JmNDlhMjQzZjAzOTdkM2UxMmE0YSIsImNvZGUiOiJBRTZ4Vk1qb2hSYVFlZWM5aWlBUnBlcjMwWGxmTDlWdFEiLCJpc3MiOiJ6bTpjaWQ6T0llNHV6OXhUWEsyYXNHamM4YWNGdyIsImdubyI6MCwidHlwZSI6MSwidGlkIjowLCJhdWQiOiJodHRwczovL29hdXRoLnpvb20udXMiLCJ1aWQiOiIwNkdEZUJhbFRIcXZhbV9VbS1QMm9RIiwibmJmIjoxNzA2MTIxODQ0LCJleHAiOjE3MTM4OTc4NDQsImlhdCI6MTcwNjEyMTg0NCwiYWlkIjoiM2NwSWxUZ3FTbkdhX0s2S1RDa0VBZyJ9.CNEFZwC6f1LXjZVCQAlJfNU4k5GJ6BKAJM3eKrnrP9_he5x5SitffOzrUv5iXDC19bLqhXKiFWCixc7alPf6wQ', 'expires_in': 3599, 'scope': 'report_chat:read:admin report:master report:read:admin meeting:read:admin'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm2Ct-ZvTP1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f955990c-245d-457a-a2c7-777f14857714"
      },
      "source": [
        "report_date= input(\"enter the report date like yyyy-mm-dd : \")\n",
        "#move2side_input=input('move2side needed ? Enter true or false: ').lower()\n",
        "move2side_input='true'\n",
        "#report_date = '2021-07-18'\n",
        "apac_report_datetime = str(report_date)+'T02'\n",
        "naol_report_datetime = str(report_date)+'T14'\n",
        "meetingID = '9181716151'\n",
        "base_url = 'https://api.zoom.us/v2/'\n",
        "past_meetings = base_url+'/past_meetings/'\n",
        "poll1_id= 'l1gX0McLRc6mxffk37nnIw'\n",
        "poll2_id = 'mlE7n7f3TQ-qgXoF--MKPQ'\n",
        "\n",
        "\n",
        "#to get result for the last held meeting\n",
        "report_meetings = base_url+'report/meetings/'\n",
        "#get_meeting_url = report_meetings+meetingID\n",
        "\n",
        "get_meeting_url = past_meetings+meetingID+'/instances'\n",
        "get_user_url = base_url+'users/'\n",
        "apac_uid=''\n",
        "naol_uid=''\n",
        "page_size ='300'\n",
        "apac_participants_list = []\n",
        "naol_participants_list = []\n",
        "apac_next_page_token = ''\n",
        "naol_next_page_token = ''\n",
        "filter_duration = 600\n",
        "\n",
        "\n",
        "current_ts = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "output_xlsx = current_ts+'_participants_list_'+report_date+\".xlsx\"\n",
        "\n",
        "\n",
        "report_datetime = datetime.strptime(report_date, '%Y-%m-%d')\n",
        "report_datetime_3week_before = report_datetime-timedelta(days=15)\n",
        "report_datetime_3month_before = report_datetime-timedelta(days=90)\n",
        "\n",
        "wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1WhACIU6F7i9VwDxXoqzx6GEwutQI5HpIYl4v6H49-7Q/edit#gid=0')\n",
        "\n",
        "input_sheet = wb.worksheet('sahajyogis')\n",
        "final_combined_sheet = wb.worksheet('combined_sheet')\n",
        "rolling_3week_sheet = wb.worksheet('Rolling3Weekend')\n",
        "weekend_archive_sheet = wb.worksheet('WeekendArchive')\n",
        "Sideroom_sheet = wb.worksheet('regular_sideroom')\n",
        "move2side_sheet = wb.worksheet('move2side')\n",
        "updated_sideroom_sheet = wb.worksheet('updated_sideroom')\n",
        "month_3_archive_frequency_sheet = wb.worksheet('3month_archive_frequency')\n",
        "\n",
        "inputdata_df = read_worksheet(input_sheet,['id','name','email'])\n",
        "#rolling_3week_df = read_worksheet(rolling_3week_sheet,['event','event_date','id','name', 'user_email', 'join_time', 'leave_time', 'duration'])\n",
        "weekend_archive_df = read_worksheet(weekend_archive_sheet,['event','event_date','id','name', 'user_email', 'join_time', 'leave_time', 'duration'])\n",
        "weekend_archive_df['event_date'] = pd.to_datetime(weekend_archive_df['event_date'])\n",
        "sideroom_data_df = read_worksheet(Sideroom_sheet,['name'])\n",
        "rolling_3week_df = weekend_archive_df[weekend_archive_df.event_date>report_datetime_3week_before]\n",
        "\n",
        "\n",
        "\n",
        "ref_id_list = list(filter(None, inputdata_df['id'].tolist()))\n",
        "ref_name_list = list(filter(None, inputdata_df['name'].tolist()))\n",
        "ref_name_list = [i.strip().lower() for i in ref_name_list]\n",
        "ref_email_list = list(filter(None, inputdata_df['email'].tolist()))\n",
        "ref_email_list = [i.strip().lower() for i in ref_email_list]\n",
        "\n",
        "\n",
        "auth_token = access_token\n",
        "\n",
        "\n",
        "\n",
        "authorization = 'Bearer ' + auth_token\n",
        "auth_headers = {\n",
        "    'Authorization': authorization,\n",
        "    'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "meeting_id_response = requests.get(get_meeting_url,headers=auth_headers)\n",
        "\n",
        "\n",
        "if meeting_id_response.status_code == 200:\n",
        "\n",
        "  for x in meeting_id_response.json()['meetings']:\n",
        "    if apac_report_datetime in str(x['start_time']):\n",
        "      apac_uid = str(x['uuid'])\n",
        "      print(x)\n",
        "    if naol_report_datetime in str(x['start_time']):\n",
        "      naol_uid = str(x['uuid'])\n",
        "      print(x)\n",
        "\n",
        "\n",
        "\n",
        "print('apac_id : '+apac_uid)\n",
        "print('naol_id : '+naol_uid)\n",
        "\n",
        "\n",
        "if apac_uid != \"\":\n",
        "  apac_encoded_uid = double_encode_uuid(apac_uid)\n",
        "  apac_participants_list = get_participants_list(apac_encoded_uid)\n",
        "  #apac_poll_answer_df = get_poll_answer_df(apac_encoded_uid)\n",
        "  #apac_poll1_df = get_poll1_df(apac_poll_answer_df,poll1_id)\n",
        "  #apac_poll2_df = get_poll1_df(apac_poll_answer_df,poll2_id)\n",
        "\n",
        "\n",
        "if naol_uid != \"\":\n",
        "  naol_encoded_uid = double_encode_uuid(naol_uid)\n",
        "  naol_participants_list = get_participants_list(naol_encoded_uid)\n",
        "  #naol_poll_answer_df = get_poll_answer_df(naol_encoded_uid)\n",
        "  #naol_poll1_df = get_poll1_df(naol_poll_answer_df,poll1_id)\n",
        "  #naol_poll2_df = get_poll1_df(naol_poll_answer_df,poll2_id)\n",
        "\n",
        "\n",
        "print(len(apac_participants_list))\n",
        "print(len(naol_participants_list))\n",
        "\n",
        "if len(apac_participants_list)>0:\n",
        "  apac_participants_df = get_participants_df(apac_participants_list,'APAC',ref_id_list,ref_name_list,ref_email_list,filter_duration)\n",
        "  #set_with_dataframe(apac_output_sheet, apac_participants_df)\n",
        "\n",
        "if len(naol_participants_list)>0:\n",
        "  naol_participants_df = get_participants_df(naol_participants_list,'NAOL',ref_id_list,ref_name_list,ref_email_list,filter_duration)\n",
        "  #set_with_dataframe(naol_output_sheet, naol_participants_df)\n",
        "\n",
        "final_combined_df = apac_participants_df.append(naol_participants_df)\n",
        "final_combined_sheet.clear()\n",
        "set_with_dataframe(final_combined_sheet, final_combined_df)\n",
        "\n",
        "if move2side_input == 'true':\n",
        "  rolling_3week_df_final = get_rolling_3week_df(rolling_3week_df,final_combined_df)\n",
        "  weekend_archive_df_final = get_weekend_archive_df(weekend_archive_df,final_combined_df)\n",
        "  weekend_archive_df_final = weekend_archive_df_final.drop_duplicates(subset=['event','event_date','name'])\n",
        "\n",
        "  weekend_archive_sheet.clear()\n",
        "  rolling_3week_sheet.clear()\n",
        "  #set_with_dataframe(rolling3week_output_sheet, rolling_3week_df_final)\n",
        "  set_with_dataframe(weekend_archive_sheet,weekend_archive_df_final)\n",
        "  set_with_dataframe(rolling_3week_sheet, rolling_3week_df_final)\n",
        "\n",
        "  weekend_archive_df_final_filtered_3m_frequency = get_weekend_archive_df_final_filtered_3m(weekend_archive_df_final)\n",
        "  set_with_dataframe(month_3_archive_frequency_sheet,weekend_archive_df_final_filtered_3m_frequency)\n",
        "\n",
        "  #archive_group_df_max_date_details = get_archive_group_df_max_date_details(weekend_archive_df_final)\n",
        "  #archive_name_list=archive_group_df_max_date_details['name'].tolist()\n",
        "  #archive_name_list_new = [i for i in archive_name_list if len(i) > 1]\n",
        "  #matched_name_list=[]\n",
        "  #for index,row in sideroom_data_df.iterrows():\n",
        "  #  highest_match = process.extractOne(row['name'],archive_name_list_new)\n",
        "  #  matched_name_list.append([row['name'],highest_match[0],highest_match[1]])\n",
        "\n",
        "  updated_sideroom_name_df = weekend_archive_df_final_filtered_3m_frequency[weekend_archive_df_final_filtered_3m_frequency['count']>3]['name']\n",
        "  updated_sideroom_name_list = updated_sideroom_name_df.to_list()\n",
        "  weekend_archive_df_final_filtered_3m = weekend_archive_df_final[weekend_archive_df_final.event_date > report_datetime_3month_before]\n",
        "  weekend_archive_df_final_filtered_3m_morethan4 = weekend_archive_df_final_filtered_3m[weekend_archive_df_final_filtered_3m.name.isin(updated_sideroom_name_list)]\n",
        "  updated_sideroom_details_df1 = weekend_archive_df_final_filtered_3m_morethan4.sort_values(by='event_date', ascending=False).drop_duplicates(subset=['name'])\n",
        "  updated_sideroom_details_df = updated_sideroom_details_df1[['name','event_date','event']].sort_values(by='name')\n",
        "\n",
        "  #updated_sideroom_details_df = get_updated_sideroom_details(matched_name_list,archive_group_df_max_date_details)\n",
        "\n",
        "  updated_sideroom_sheet.clear()\n",
        "  set_with_dataframe(updated_sideroom_sheet, updated_sideroom_details_df)\n",
        "  #set_with_dataframe(updated_regular_output_sheet, updated_sideroom_details_df)\n",
        "\n",
        "  final_max_count_filtered = pivot_rolling_3week_df(rolling_3week_df_final)\n",
        "\n",
        "  regular_matched_name_list=updated_sideroom_details_df['name'].tolist()\n",
        "  regular_matched_name_list_new = [i for i in regular_matched_name_list if len(i) > 1]\n",
        "  move2side_name_list=[]\n",
        "  for index,row in final_max_count_filtered.iterrows():\n",
        "    highest_match_move2side = process.extractOne(row['name'],regular_matched_name_list_new)\n",
        "    move2side_name_list.append([row['name'],highest_match_move2side[0],highest_match_move2side[1]])\n",
        "\n",
        "  move2side_final = get_move2side_final(move2side_name_list,final_max_count_filtered)\n",
        "\n",
        "  email_name_ref_df = get_email_name_ref_df(weekend_archive_df_final)\n",
        "\n",
        "  move2side_final_df_with_email = pd.merge(move2side_final,email_name_ref_df,how='left',on='name')\n",
        "  move2side_final_df_with_email.drop_duplicates(inplace=True)\n",
        "\n",
        "  #email_list = updated_sideroom_details_df['user_email'].to_list()\n",
        "  #email_list = list(filter(None, email_list))\n",
        "  #email_list = [i.lower() for i in email_list]\n",
        "\n",
        "  #move2side_final_df_with_email = move2side_final_df_with_email[~(move2side_final_df_with_email.user_email).str.lower().isin(email_list)]\n",
        "\n",
        "  move2side_sheet.clear()\n",
        "  set_with_dataframe(move2side_sheet, move2side_final_df_with_email)\n",
        "  #set_with_dataframe(move2side_output_sheet,move2side_final_df_with_email)\n",
        "\n",
        "with ExcelWriter(output_xlsx) as writer:\n",
        "  final_combined_df.to_excel(writer, sheet_name=\"final_combined\")\n",
        "  apac_participants_df.to_excel(writer, sheet_name=\"APAC\")\n",
        "  naol_participants_df.to_excel(writer, sheet_name=\"NAOL\")\n",
        "  rolling_3week_df_final.to_excel(writer, sheet_name=\"3weekRollingWeekend\")\n",
        "  move2side_final_df_with_email.to_excel(writer, sheet_name=\"move2side\")\n",
        "  updated_sideroom_details_df.to_excel(writer, sheet_name=\"updated_regular_sideroom\")\n",
        "  #apac_poll1_df.to_excel(writer, sheet_name=\"APAC_poll1\")\n",
        "  #apac_poll2_df.to_excel(writer, sheet_name=\"APAC_poll2\")\n",
        "  #naol_poll1_df.to_excel(writer, sheet_name=\"NAOL_poll1\")\n",
        "  #naol_poll2_df.to_excel(writer, sheet_name=\"NAOL_poll2\")\n",
        "\n",
        "#files.download(output_xlsx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the report date like yyyy-mm-dd : 2024-01-21\n",
            "{'uuid': '5Ja++uPqRxqZizgE5Pk9iQ==', 'start_time': '2024-01-21T14:06:28Z'}\n",
            "{'uuid': 'xmUlGN3OToqCJtjVqnJGFA==', 'start_time': '2024-01-21T02:30:16Z'}\n",
            "apac_id : xmUlGN3OToqCJtjVqnJGFA==\n",
            "naol_id : 5Ja++uPqRxqZizgE5Pk9iQ==\n",
            "135\n",
            "196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-dbdd28bfac02>:121: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  final_combined_df = apac_participants_df.append(naol_participants_df)\n",
            "<ipython-input-2-e8f2b41925fb>:80: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  rolling_3week_df_combined = rolling_3week_df.append(final_combined_df)\n",
            "<ipython-input-2-e8f2b41925fb>:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  weekend_archive_df_final = weekend_archive_df.append(final_combined_df)\n",
            "<ipython-input-2-e8f2b41925fb>:151: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_max_count_filtered.drop(columns=['count'],inplace=True)\n",
            "<ipython-input-2-e8f2b41925fb>:140: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  email_name_ref_df.replace(\"\", nan_value, inplace=True)\n",
            "<ipython-input-2-e8f2b41925fb>:141: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  email_name_ref_df.dropna(inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in meeting_id_response.json()['meetings']:\n",
        "    if \"2023-11-05\" in str(x['start_time']):\n",
        "      apac_uid = str(x['uuid'])\n",
        "      print(x)\n",
        "    if naol_report_datetime in str(x['start_time']):\n",
        "      naol_uid = str(x['uuid'])\n",
        "      #print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZxvXvz6OfPe",
        "outputId": "42fdaf66-c6eb-4339-b700-7dd959765179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uuid': '9/ViHs+NQQC26D6rqITADg==', 'start_time': '2023-11-05T01:18:24Z'}\n",
            "{'uuid': 'uOfzOl9uQ4SKg5JDsHoxfg==', 'start_time': '2023-11-05T14:07:30Z'}\n"
          ]
        }
      ]
    }
  ]
}